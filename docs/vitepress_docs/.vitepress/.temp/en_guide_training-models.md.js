import { ssrRenderAttrs } from "vue/server-renderer";
import { useSSRContext } from "vue";
import { _ as _export_sfc } from "./plugin-vue_export-helper.1tPrXgE0.js";
const __pageData = JSON.parse('{"title":"Training AI Models for Open Ticket AI","description":"Guide on how to train or fine-tune models for Open Ticket AI.","frontmatter":{"title":"Training AI Models for Open Ticket AI","description":"Guide on how to train or fine-tune models for Open Ticket AI."},"headers":[],"relativePath":"en/guide/training-models.md","filePath":"en/guide/training-models.md"}');
const _sfc_main = { name: "en/guide/training-models.md" };
function _sfc_ssrRender(_ctx, _push, _parent, _attrs, $props, $setup, $data, $options) {
  _push(`<div${ssrRenderAttrs(_attrs)}><h1 id="training-the-model" tabindex="-1">Training the Model <a class="header-anchor" href="#training-the-model" aria-label="Permalink to &quot;Training the Model&quot;">​</a></h1><p>:::note You only need to train your own model if you want to use a custom model or fine-tune the default models. To achieve good results, a significant amount of training data is necessary—ideally, at least 1000 tickets per queue and priority. Data cleaning, normalization, tokenization, and experimentation with different models and hyperparameters are often required. :::</p><p>Training or fine-tuning a model for Open Ticket AI involves several key steps:</p><h3 id="_1-data-collection" tabindex="-1">1. Data Collection <a class="header-anchor" href="#_1-data-collection" aria-label="Permalink to &quot;1. Data Collection&quot;">​</a></h3><ul><li><strong>Export Historical Data</strong>: Gather historical ticket data (including subject and body) from your existing ticket system.</li><li><strong>Label Data</strong>: Accurately label each ticket with the correct queue and priority. This labeled dataset will be the foundation for training your model.</li></ul><h3 id="_2-data-cleaning" tabindex="-1">2. Data Cleaning <a class="header-anchor" href="#_2-data-cleaning" aria-label="Permalink to &quot;2. Data Cleaning&quot;">​</a></h3><ul><li><strong>Remove Noise</strong>: Eliminate irrelevant information such as email signatures, Personally Identifiable Information (PII), and spam.</li><li><strong>Normalize Text</strong>: Standardize whitespace and ensure consistent character encodings.</li></ul><h3 id="_3-data-transformation-tokenization" tabindex="-1">3. Data Transformation &amp; Tokenization <a class="header-anchor" href="#_3-data-transformation-tokenization" aria-label="Permalink to &quot;3. Data Transformation &amp; Tokenization&quot;">​</a></h3><ul><li><strong>Combine Fields</strong>: Concatenate the subject and body of the tickets to form a single input text for the model.</li><li><strong>Set <code>max_length</code></strong>: Choose an appropriate <code>max_length</code> for tokenization (e.g., 256–512 tokens) based on the median length of your tickets. This prevents truncation of important information while managing computational resources.</li><li><strong>Use Tokenizer</strong>: Utilize the provided <code>ticket_combined_email_tokenizer</code> or a tokenizer compatible with your chosen model.</li></ul><h3 id="_4-model-selection-hardware" tabindex="-1">4. Model Selection &amp; Hardware <a class="header-anchor" href="#_4-model-selection-hardware" aria-label="Permalink to &quot;4. Model Selection &amp; Hardware&quot;">​</a></h3><p>Consider the following when selecting a model and the required hardware:</p><table tabindex="0"><thead><tr><th>Model</th><th>RAM Required</th><th>Notes</th></tr></thead><tbody><tr><td><code>distilbert-base-german-cased</code></td><td>2 GB</td><td>Lightweight, German text</td></tr><tr><td><code>bert-base-german-cased</code></td><td>4 GB</td><td>Higher accuracy, German text</td></tr><tr><td><code>deberta-large-mnli</code></td><td>8 GB</td><td>Multilingual / large contexts</td></tr></tbody></table><p>(<em>Note: The table above provides examples; actual requirements may vary.</em>)</p><h3 id="_5-training-fine-tuning" tabindex="-1">5. Training &amp; Fine-Tuning <a class="header-anchor" href="#_5-training-fine-tuning" aria-label="Permalink to &quot;5. Training &amp; Fine-Tuning&quot;">​</a></h3><ul><li><strong>Hyperparameter Tuning</strong>: Use the built-in hyperparameter tuner to experiment with settings like <code>learning_rate</code>, <code>batch_size</code>, and <code>epochs</code> to find the optimal configuration for your dataset.</li><li><strong>Monitor Performance</strong>: The training script will output a summary of the model&#39;s performance and resource usage, helping you track progress and make adjustments.</li></ul><h3 id="_6-evaluation" tabindex="-1">6. Evaluation <a class="header-anchor" href="#_6-evaluation" aria-label="Permalink to &quot;6. Evaluation&quot;">​</a></h3><ul><li><strong>Measure Accuracy</strong>: Evaluate the model&#39;s performance based on its accuracy in predicting queue and priority.</li><li><strong>Analyze Confidence</strong>: Examine the relationship between the model&#39;s confidence scores and the correctness of its predictions. This analysis is crucial for setting an optimal <code>confidence_threshold</code> in the configuration.</li><li><strong>Confidence-Weighted Accuracy Score (CWAS)</strong> (Optional): You might consider using a metric like CWAS to get a more nuanced view of performance:<div class="language-math vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">math</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>\\text{CWAS} = \\text{percent_predicted} \\times (\\text{percent_correct}^2)</span></span></code></pre></div>This score rewards models that are both accurate and make a high percentage of predictions (i.e., are not overly reliant on falling back to a default for low-confidence cases).</li></ul></div>`);
}
const _sfc_setup = _sfc_main.setup;
_sfc_main.setup = (props, ctx) => {
  const ssrContext = useSSRContext();
  (ssrContext.modules || (ssrContext.modules = /* @__PURE__ */ new Set())).add("en/guide/training-models.md");
  return _sfc_setup ? _sfc_setup(props, ctx) : void 0;
};
const trainingModels = /* @__PURE__ */ _export_sfc(_sfc_main, [["ssrRender", _sfc_ssrRender]]);
export {
  __pageData,
  trainingModels as default
};
